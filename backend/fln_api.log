2025-08-22 22:03:32,613 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-22 22:03:32,616 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-22 22:04:10,025 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-22 22:04:10,028 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-22 22:05:04,942 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-22 22:05:04,944 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-22 22:05:49,222 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-22 22:05:49,225 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-22 22:06:16,329 [INFO] FLN-API - Received /generate_plan request
2025-08-22 22:06:16,523 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-22 22:06:48,024 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-22 22:06:48,084 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-22 22:06:48,176 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-22 22:06:58,345 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-22 22:06:58,359 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-22 22:06:58,428 [INFO] FLN-API - Successfully generated plan JSON.
2025-08-22 23:13:31,868 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-22 23:13:31,908 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-22 23:13:49,889 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-22 23:13:49,893 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-22 23:14:15,820 [INFO] FLN-API - Received /generate_plan request
2025-08-22 23:14:16,064 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-22 23:14:35,300 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-22 23:14:35,308 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-22 23:14:35,382 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-22 23:14:52,095 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-22 23:14:52,097 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-22 23:14:52,185 [INFO] FLN-API - Successfully generated plan JSON.
2025-08-22 23:19:31,620 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-22 23:19:31,623 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-22 23:19:50,643 [INFO] FLN-API - Received /generate_plan request
2025-08-22 23:19:50,961 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-22 23:20:26,074 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-22 23:20:26,081 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-22 23:20:26,182 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-22 23:20:35,929 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-22 23:20:35,931 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-22 23:20:36,004 [INFO] FLN-API - Successfully generated plan JSON.
2025-08-22 23:23:17,033 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-22 23:23:17,036 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-22 23:25:17,279 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-22 23:25:17,283 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-22 23:26:56,443 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-22 23:26:56,446 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-22 23:27:30,040 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-22 23:27:30,043 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-23 00:50:50,502 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-23 00:50:50,506 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-23 01:26:52,364 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-23 01:26:52,369 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-23 01:37:56,216 [INFO] FLN-API - Received /generate_plan request
2025-08-23 01:37:57,666 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-23 01:38:22,817 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-23 01:38:22,843 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-23 01:38:23,031 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-23 01:38:24,101 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 500 Internal Server Error"
2025-08-23 01:38:24,220 [ERROR] FLN-API - Error during plan generation
Traceback (most recent call last):
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\litellm\llms\vertex_ai\gemini\vertex_and_google_ai_studio_gemini.py", line 1890, in completion
    response = client.post(url=url, headers=headers, json=data)  # type: ignore
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 780, in post
    raise e
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 762, in post
    response.raise_for_status()
    ~~~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\httpx\_models.py", line 829, in raise_for_status
    raise HTTPStatusError(message, request=request, response=self)
httpx.HTTPStatusError: Server error '500 Internal Server Error' for url 'https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ'
For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\litellm\main.py", line 2606, in completion
    response = vertex_chat_completion.completion(  # type: ignore
        model=model,
    ...<17 lines>...
        extra_headers=extra_headers,
    )
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\litellm\llms\vertex_ai\gemini\vertex_and_google_ai_studio_gemini.py", line 1894, in completion
    raise VertexAIError(
    ...<3 lines>...
    )
litellm.llms.vertex_ai.common_utils.VertexAIError: {
  "error": {
    "code": 500,
    "message": "An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting",
    "status": "INTERNAL"
  }
}


During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\backend\app.py", line 144, in generate_plan
    result = crew.kickoff()
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
        agent=agent_to_use,
        context=context,
        tools=cast(List[BaseTool], tools_for_task),
    )
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\task.py", line 364, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\task.py", line 512, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\task.py", line 428, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\agent.py", line 458, in execute_task
    raise e
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\agent.py", line 434, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\agent.py", line 530, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\llm.py", line 1019, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 1345, in exception_type
    raise litellm.InternalServerError(
    ...<9 lines>...
    )
litellm.exceptions.InternalServerError: litellm.InternalServerError: VertexAIException InternalServerError - {
  "error": {
    "code": 500,
    "message": "An internal error has occurred. Please retry or report in https://developers.generativeai.google/guide/troubleshooting",
    "status": "INTERNAL"
  }
}

2025-08-23 01:39:01,189 [INFO] FLN-API - Received /generate_plan request
2025-08-23 01:39:01,892 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-23 01:39:45,794 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-23 01:39:45,800 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-23 01:39:46,151 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-23 01:39:48,044 [ERROR] FLN-API - Error during plan generation
Traceback (most recent call last):
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\httpx\_transports\default.py", line 101, in map_httpcore_exceptions
    yield
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\httpx\_transports\default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 256, in handle_request
    raise exc from None
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\httpcore\_sync\connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
        pool_request.request
    )
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\httpcore\_sync\connection.py", line 103, in handle_request
    return self._connection.handle_request(request)
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 136, in handle_request
    raise exc
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 106, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 177, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\httpcore\_sync\http11.py", line 217, in _receive_event
    data = self._network_stream.read(
        self.READ_NUM_BYTES, timeout=timeout
    )
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\httpcore\_backends\sync.py", line 126, in read
    with map_exceptions(exc_map):
         ~~~~~~~~~~~~~~^^^^^^^^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\Lib\contextlib.py", line 162, in __exit__
    self.gen.throw(value)
    ~~~~~~~~~~~~~~^^^^^^^
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\httpcore\_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ReadError: [WinError 10054] An existing connection was forcibly closed by the remote host

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\litellm\main.py", line 2606, in completion
    response = vertex_chat_completion.completion(  # type: ignore
        model=model,
    ...<17 lines>...
        extra_headers=extra_headers,
    )
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\litellm\llms\vertex_ai\gemini\vertex_and_google_ai_studio_gemini.py", line 1890, in completion
    response = client.post(url=url, headers=headers, json=data)  # type: ignore
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 782, in post
    raise e
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\litellm\llms\custom_httpx\http_handler.py", line 761, in post
    response = self.client.send(req, stream=stream)
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\httpx\_client.py", line 914, in send
    response = self._send_handling_auth(
        request,
    ...<2 lines>...
        history=[],
    )
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\httpx\_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
        request,
        follow_redirects=follow_redirects,
        history=history,
    )
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\httpx\_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\httpx\_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\httpx\_transports\default.py", line 249, in handle_request
    with map_httpcore_exceptions():
         ~~~~~~~~~~~~~~~~~~~~~~~^^
  File "C:\Program Files\WindowsApps\PythonSoftwareFoundation.Python.3.13_3.13.2032.0_x64__qbz5n2kfra8p0\Lib\contextlib.py", line 162, in __exit__
    self.gen.throw(value)
    ~~~~~~~~~~~~~~^^^^^^^
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\httpx\_transports\default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ReadError: [WinError 10054] An existing connection was forcibly closed by the remote host

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\backend\app.py", line 144, in generate_plan
    result = crew.kickoff()
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\crew.py", line 661, in kickoff
    result = self._run_sequential_process()
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\crew.py", line 772, in _run_sequential_process
    return self._execute_tasks(self.tasks)
           ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\crew.py", line 875, in _execute_tasks
    task_output = task.execute_sync(
        agent=agent_to_use,
        context=context,
        tools=cast(List[BaseTool], tools_for_task),
    )
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\task.py", line 364, in execute_sync
    return self._execute_core(agent, context, tools)
           ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\task.py", line 512, in _execute_core
    raise e  # Re-raise the exception after emitting the event
    ^^^^^^^
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\task.py", line 428, in _execute_core
    result = agent.execute_task(
        task=self,
        context=context,
        tools=tools,
    )
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\agent.py", line 458, in execute_task
    raise e
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\agent.py", line 434, in execute_task
    result = self._execute_without_timeout(task_prompt, task)
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\agent.py", line 530, in _execute_without_timeout
    return self.agent_executor.invoke(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        {
        ^
    ...<4 lines>...
        }
        ^
    )["output"]
    ^
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 114, in invoke
    formatted_answer = self._invoke_loop()
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 208, in _invoke_loop
    raise e
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\agents\crew_agent_executor.py", line 154, in _invoke_loop
    answer = get_llm_response(
        llm=self.llm,
    ...<3 lines>...
        from_task=self.task
    )
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 160, in get_llm_response
    raise e
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\utilities\agent_utils.py", line 153, in get_llm_response
    answer = llm.call(
        messages,
    ...<2 lines>...
        from_agent=from_agent,
    )
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\llm.py", line 1019, in call
    return self._handle_non_streaming_response(
           ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        params, callbacks, available_functions, from_task, from_agent
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\crewai\llm.py", line 813, in _handle_non_streaming_response
    response = litellm.completion(**params)
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\litellm\utils.py", line 1330, in wrapper
    raise e
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\litellm\utils.py", line 1205, in wrapper
    result = original_function(*args, **kwargs)
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\litellm\main.py", line 3427, in completion
    raise exception_type(
          ~~~~~~~~~~~~~~^
        model=model,
        ^^^^^^^^^^^^
    ...<3 lines>...
        extra_kwargs=kwargs,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2301, in exception_type
    raise e
  File "C:\Users\panka\Desktop\Cograd\FLN_Assesment_plan\.venv\Lib\site-packages\litellm\litellm_core_utils\exception_mapping_utils.py", line 2270, in exception_type
    raise APIConnectionError(
    ...<4 lines>...
    )
litellm.exceptions.APIConnectionError: litellm.APIConnectionError: GeminiException - [WinError 10054] An existing connection was forcibly closed by the remote host
2025-08-23 01:39:48,500 [ERROR] crewai.telemetry.telemetry - HTTPSConnectionPool(host='telemetry.crewai.com', port=4319): Max retries exceeded with url: /v1/traces (Caused by NameResolutionError("<urllib3.connection.HTTPSConnection object at 0x000001C10200B750>: Failed to resolve 'telemetry.crewai.com' ([Errno 11001] getaddrinfo failed)"))
2025-08-23 01:41:12,985 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-23 01:41:12,988 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-23 01:41:23,304 [INFO] FLN-API - Received /generate_plan request
2025-08-23 01:41:23,966 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-23 01:42:03,028 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-23 01:42:03,035 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-23 01:42:03,235 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-23 01:42:14,987 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-23 01:42:14,990 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-23 01:42:15,062 [INFO] FLN-API - Successfully generated plan JSON.
2025-08-23 01:45:33,889 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-23 01:45:33,890 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-23 01:45:45,097 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-23 01:45:45,101 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-23 01:46:01,622 [INFO] FLN-API - Received /generate_plan request
2025-08-23 01:46:02,261 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-23 01:47:20,310 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-23 01:47:20,320 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-23 01:47:20,439 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-23 01:47:37,647 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-23 01:47:37,650 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-23 01:47:37,754 [INFO] FLN-API - Successfully generated plan JSON.
2025-08-23 01:56:18,866 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-23 01:56:18,869 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-23 01:58:12,874 [INFO] FLN-API - Received /generate_plan request
2025-08-23 01:58:13,439 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-23 01:58:29,324 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-23 01:58:29,337 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-23 01:58:29,456 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-23 01:58:35,959 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-23 01:58:35,964 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-23 01:58:36,105 [INFO] FLN-API - Successfully generated plan JSON.
2025-08-23 02:00:41,000 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-23 02:00:41,002 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-23 02:02:57,322 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-23 02:02:57,324 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-23 02:03:13,716 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-23 02:03:13,719 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-23 02:06:20,048 [INFO] FLN-API - Received /generate_plan request
2025-08-23 02:06:20,577 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-23 02:06:52,352 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-23 02:06:52,359 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-23 02:06:52,498 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-23 02:07:50,322 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-23 02:07:50,329 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-23 02:07:50,612 [INFO] FLN-API - Successfully generated plan JSON.
2025-08-23 02:08:14,525 [INFO] FLN-API - Received /generate_plan request
2025-08-23 02:08:14,820 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-23 02:08:27,453 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-23 02:08:27,457 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-23 02:08:27,556 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-23 02:08:35,132 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-23 02:08:35,137 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-23 02:08:35,270 [INFO] FLN-API - Successfully generated plan JSON.
2025-08-23 02:10:11,212 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-23 02:10:11,218 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-23 02:11:00,497 [INFO] FLN-API - Received /generate_plan request
2025-08-23 02:11:01,746 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-23 02:11:22,459 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-23 02:11:22,475 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-23 02:11:22,725 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-23 02:11:34,539 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-23 02:11:34,541 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-23 02:11:34,623 [INFO] FLN-API - Successfully generated plan JSON.
2025-08-23 02:14:05,550 [INFO] FLN-API - Loaded GEMINI_API_KEY from environment.
2025-08-23 02:14:05,552 [INFO] FLN-API - Learning outcomes JSON loaded successfully.
2025-08-23 02:14:28,619 [INFO] FLN-API - Received /generate_plan request
2025-08-23 02:14:30,171 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-23 02:14:52,576 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-23 02:14:52,583 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-23 02:14:52,766 [INFO] LiteLLM - 
LiteLLM completion() model= gemini-2.5-flash-lite; provider = gemini
2025-08-23 02:15:03,242 [INFO] httpx - HTTP Request: POST https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-lite:generateContent?key=AIzaSyBXvBDm35vek1rQcsRlO9koawpOqyoXCoQ "HTTP/1.1 200 OK"
2025-08-23 02:15:03,333 [INFO] LiteLLM - Wrapper: Completed Call, calling success_handler
2025-08-23 02:15:03,467 [INFO] FLN-API - Successfully generated plan JSON.
